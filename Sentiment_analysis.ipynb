{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import ast     \n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing dataset \n",
    "raw_data = pd.read_csv('twitter_data.tsv',delimiter= '\\t', quoting=3, names=['tweet','device','location','candidate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_twt = pd.read_csv('Data/Tweets_by_candidates.tsv',delimiter= '\\t', quoting=3, names=['tweet','candidate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Streamed Tweets\n",
    "#Dropping Duplicates\n",
    "\n",
    "raw_data = raw_data.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "#Creating columns for Sentiment Scores\n",
    "raw_data['Sentiment_Compund']=np.nan\n",
    "raw_data['Sentiment_Neg']=np.nan\n",
    "raw_data['Sentiment_Neu']=np.nan\n",
    "raw_data['Sentiment_Pos']=np.nan\n",
    "\n",
    "raw_data['ht'] = np.nan\n",
    "raw_data['mention'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Candidates tweets\n",
    "#Dropping Duplicates\n",
    "cand_twt = cand_twt.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "#Creating columns for Sentiment Scores\n",
    "cand_twt['Sentiment_Compund']=np.nan\n",
    "cand_twt['Sentiment_Neg']=np.nan\n",
    "cand_twt['Sentiment_Neu']=np.nan\n",
    "cand_twt['Sentiment_Pos']=np.nan\n",
    "\n",
    "cand_twt['ht'] = np.nan\n",
    "cand_twt['mention'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting Hashtags and Mentions from Streamed tweets\n",
    "\n",
    "for i in range(len(raw_data)):\n",
    "    hashtag = re.findall(r'#(\\w+)', raw_data['tweet'][i])\n",
    "    if len(hashtag) > 0 :\n",
    "        raw_data['ht'][i] = hashtag\n",
    "    mention = re.findall(r'@(\\w+)', raw_data['tweet'][i])\n",
    "    if len(hashtag) > 0 :\n",
    "        raw_data['mention'][i] = mention\n",
    " \n",
    "raw_data = raw_data.fillna(\"None\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting Hashtags and Mentions from Candidates Tweets\n",
    "\n",
    "for i in range(len(cand_twt)):\n",
    "    hashtag = re.findall(r'#(\\w+)', cand_twt['tweet'][i])\n",
    "    if len(hashtag) > 0 :\n",
    "        cand_twt['ht'][i] = hashtag\n",
    "    mention = re.findall(r'@(\\w+)', cand_twt['tweet'][i])\n",
    "    if len(hashtag) > 0 :\n",
    "        cand_twt['mention'][i] = mention\n",
    " \n",
    "cand_twt = cand_twt.fillna(\"None\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting State name from location \n",
    "\n",
    "#Reading state files\n",
    "states = pd.read_csv('states.tsv',delimiter= '\\t', \n",
    "                       quoting=3, \n",
    "                       names=['State','Abbrv.','PS']) \n",
    "states = states.drop([0]).reset_index(drop=True)\n",
    "\n",
    "#Creating dictionary for states\n",
    "states_dict = dict()\n",
    "for i in range(len(states)):\n",
    "    states_dict[states['PS'][i]] = states['State'][i] \n",
    "\n",
    "raw_data['state']= np.nan\n",
    "\n",
    "for i in range(len(raw_data)):\n",
    "    if( (i+1)%10000 == 0 ):\n",
    "        print(\"%d of %d Tweets has been Processed\" % ( i+1, len(raw_data)))  \n",
    "    loc = raw_data['location'][i].lower()\n",
    "    for k, v in states_dict.items():\n",
    "        flag = False\n",
    "        if (k in raw_data.iloc[i ,2]):\n",
    "            raw_data['state'][i]= k\n",
    "            flag = True\n",
    "        if ( not flag and v in raw_data.iloc[i ,2]):\n",
    "            raw_data['state'][i]= k\n",
    "\n",
    "#Checking observations with no state information\n",
    "raw_data['state'].isnull().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis using VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyzing Sentiment using Vader\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "# For streamed tweets \n",
    "for i in range(len(raw_data)):\n",
    "    if( (i+1)%10000 == 0 ):\n",
    "        print(\"%d of %d Tweets has been analyzed\" % ( i+1, len(raw_data)))   \n",
    "    score = analyser.polarity_scores(raw_data['tweet'][i])\n",
    "    raw_data['Sentiment_Compund'][i]=score['compound']\n",
    "    raw_data['Sentiment_Neg'][i]=score['neg']\n",
    "    raw_data['Sentiment_Neu'][i]=score['neu']\n",
    "    raw_data['Sentiment_Pos'][i]=score['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tweets by candidates\n",
    "\n",
    "for i in range(len(cand_twt)):\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "        print(\"%d of %d Tweets has been analyzed\" % ( i+1, len(cand_twt)))   \n",
    "    score = analyser.polarity_scores(cand_twt['tweet'][i])\n",
    "    cand_twt['Sentiment_Compund'][i]=score['compound']\n",
    "    cand_twt['Sentiment_Neg'][i]=score['neg']\n",
    "    cand_twt['Sentiment_Neu'][i]=score['neu']\n",
    "    cand_twt['Sentiment_Pos'][i]=score['pos']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing data to file for backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for i in range(len(raw_data)):\n",
    "    f = open(\"Tweets.tsv\", \"a\")\n",
    "    f.write(raw_data['device'][i]+\"\\t\"+raw_data['candidate'][i]+\"\\t\"+\n",
    "            str(raw_data['Sentiment_Compund'][i])+\"\\t\"+\n",
    "            str(raw_data['Sentiment_Neg'][i])+\"\\t\"+str(raw_data['Sentiment_Neu'][i])+\"\\t\"+\n",
    "            str(raw_data['Sentiment_Pos'][i])+\"\\t\"+\n",
    "            str(raw_data['ht'][i])+\"\\t\"+\n",
    "            str(raw_data['mention'][i])+\"\\t\"+\n",
    "            str(raw_data['state'][i])+\"\\n\")\n",
    "    f.close()    \n",
    "\n",
    "for i in range(len(cand_twt)):\n",
    "    f = open(\"Cand_tweets.tsv\", \"a\")\n",
    "    f.write(cand_twt['candidate'][i]+\"\\t\"+\n",
    "            str(cand_twt['Sentiment_Compund'][i])+\"\\t\"+\n",
    "            str(cand_twt['Sentiment_Neg'][i])+\"\\t\"+str(cand_twt['Sentiment_Neu'][i])+\"\\t\"+\n",
    "            str(cand_twt['Sentiment_Pos'][i])+\"\\t\"+\n",
    "            str(cand_twt['ht'][i])+\"\\t\"+\n",
    "            str(cand_twt['mention'][i])+\"\\n\")\n",
    "    f.close()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_in = pd.read_csv('Tweets.tsv',delimiter= '\\t', \n",
    "                       quoting=3, \n",
    "                       names=['device','candidate','Sentiment_Compund',\n",
    "                              'Sentiment_Neg','Sentiment_Neu','Sentiment_Pos',\n",
    "                              'ht','mention'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_tweets_in = pd.read_csv('Cand_tweets.tsv',delimiter= '\\t', \n",
    "                       quoting=3, \n",
    "                       names=['candidate','Sentiment_Compund',\n",
    "                              'Sentiment_Neg','Sentiment_Neu','Sentiment_Pos',\n",
    "                              'ht','mention']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking for most used hashtags in the tweets\n",
    "dict_one = dict()    \n",
    "\n",
    "for i in range(len(tweets_in)):\n",
    "    if tweets_in['ht'][i] != 'None':\n",
    "        res = ast.literal_eval(tweets_in['ht'][i])\n",
    "        for j in res:\n",
    "            word = j.lower()\n",
    "            dict_one[word] = dict_one.get(word,0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking for most used mention in the tweets\n",
    "dict_two = dict()    \n",
    "\n",
    "for i in range(len(tweets_in)):\n",
    "    if tweets_in['mention'][i] != 'None':\n",
    "        res = ast.literal_eval(tweets_in['mention'][i])\n",
    "        if len(res) > 0:\n",
    "            for j in res:\n",
    "                word = j.lower()\n",
    "                dict_two[word] = dict_two.get(word,0)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis (Using Bag of Words Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data  \n",
    "cols = ['sentiment','id','date','query_string','user','text']\n",
    "sentiment140_df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\",header=None, names=cols,encoding='iso-8859-1')\n",
    "\n",
    "sentiment140_df.head()\n",
    "\n",
    "sentiment140_df.sentiment.value_counts()\n",
    "sentiment140_df.drop(['id','date','query_string','user'],axis=1,inplace=True)\n",
    "\n",
    "sentiment140_df[sentiment140_df.sentiment == 0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "data_dict = {\n",
    "    'sentiment':{\n",
    "        'type':train.sentiment.dtype,\n",
    "        'description':'sentiment class - 0:negative, 1:positive'\n",
    "    },\n",
    "    'text':{\n",
    "        'type':train.text.dtype,\n",
    "        'description':'tweet text'\n",
    "    },\n",
    "    'pre_clean_len':{\n",
    "        'type':train.pre_clean_len.dtype,\n",
    "        'description':'Length of the tweet before cleaning'\n",
    "    },\n",
    "    'dataset_shape':train.shape\n",
    "}\n",
    "pprint(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and parsing tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = WordPunctTokenizer()\n",
    "mention_reg = r'@[A-Za-z0-9]+'\n",
    "url_reg = r'https?://[A-Za-z0-9./]+'\n",
    "combined_reg = r'|'.join((mention_reg, url_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_reg, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "clean_tweet_texts = []\n",
    "for i in range(len(train)):\n",
    "    if( (i+1)%10000 == 0 ):\n",
    "        print(\"Tweets %d of %d has been processed\" % ( i+1,len(train)))                                                                   \n",
    "    clean_tweet_texts.append(tweet_cleaner(train['text'][i]))\n",
    "\n",
    "s140_df = pd.DataFrame(clean_tweet_texts,columns=['text'])\n",
    "s140_df['target'] = train.sentiment\n",
    "s140_df.head()\n",
    "s140_df.to_csv('clean_sentiment140.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemmming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(nums[4]):\n",
    "    if( (i+1)%10000 == 0 ):\n",
    "        print(\"Tweets %d of %d has been processed\" % ( i+1, nums[4])) \n",
    "    tweet140 = str(s140_df['text'][i])\n",
    "    tweet140 = tweet140.lower()\n",
    "    tweet140 = tweet140.split()\n",
    "    ps = PorterStemmer()\n",
    "    tweet140 = [ps.stem(word) for word in tweet140 if not word in set(stopwords.words('english'))]\n",
    "    tweet140 = ' '.join(tweet140)\n",
    "    corpus.append(tweet140)\n",
    "\n",
    "train140 = pd.DataFrame(corpus,columns=['text'])\n",
    "train140['sentiment'] = my_df.target\n",
    "train140.head()\n",
    "train140.to_csv('final_train.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training (Using Counter Vectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train140.dropna(inplace = True)\n",
    "\n",
    "cv = CountVectorizer(max_features = 1800)\n",
    "X = cv.fit_transform(train140.text).toarray()\n",
    "y = train140.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training (Using Hashing Vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.iloc[:, 0].values\n",
    "y = train_data.iloc[:, 1].values\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the transform\n",
    "vectorizer = HashingVectorizer(n_features=200)\n",
    "# encode document\n",
    "vector = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# summarize encoded vector\n",
    "print(vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_hash = GaussianNB()\n",
    "classifier_hash.fit(vector.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_test = vectorizer.transform(X_test)\n",
    "print(vector_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier_hash.predict(vector_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training (Using Hashing Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer(max_features=500, min_df=7, max_df=0.8, stop_words=stopwords.words('english'))\n",
    "\n",
    "# tokenize and build vocab\n",
    "vector = vectorizer_tfidf.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(vector)\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_tfidf = GaussianNB()\n",
    "classifier_tfidf.fit(vector.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_test = vectorizer_tfidf.transform(X_test)\n",
    "print(vector_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier_hash.predict(vector_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "wc = WordCloud(background_color=\"white\", max_words=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data =  pd.read_csv('Tweets_by_candidates.tsv',delimiter= '\\t', quoting=3, names=['tweet','candidate'])\n",
    "\n",
    "\n",
    "joe_words = test_data[test_data['candidate']== 'Joe_Biden']\n",
    "warren_words = test_data[test_data['candidate']== 'Elizabeth_Warren']\n",
    "sanders_words = test_data[test_data['candidate']== 'Bernie_Sanders']\n",
    "booker_words = test_data[test_data['candidate']== 'Cory_Booker']\n",
    "yang_words = test_data[test_data['candidate']== 'Andrew_Yang']\n",
    "pete_words = test_data[test_data['candidate']== 'Pete_Buttigieg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for line in joe_words['tweet']:\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        corpus.append(word)\n",
    "\n",
    "text =' '.join(corpus)\n",
    "\n",
    "text = re.sub(r'@[A-Za-z0-9]+','',text)\n",
    "text = re.sub('https?://[A-Za-z0-9./]+','',text)\n",
    "text = re.sub('rt','',text)\n",
    "text = re.sub('RT','',text)\n",
    "text = re.sub('amp','',text)\n",
    "text = re.sub('will','',text)\n",
    "text = re.sub('new','',text)\n",
    "text = re.sub('today','',text)\n",
    "\n",
    "wc.generate(text)\n",
    "wc.to_file('wc_joe.jpeg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
